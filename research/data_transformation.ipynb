{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/config/workspace/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/config/workspace'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassification.constants import *\n",
    "from textClassification.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /config/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.stemmer = nltk.SnowballStemmer(\"english\")\n",
    "        self.stopword = set(stopwords.words('english'))\n",
    "\n",
    "    \n",
    "    # Let's apply regex and do cleaning.\n",
    "    def data_cleaning(self,words):\n",
    "        words = str(words).lower()\n",
    "        words = re.sub('\\[.*?\\]', '', words)\n",
    "        words = re.sub('https?://\\S+|www\\.\\S+', '', words)\n",
    "        words = re.sub('<.*?>+', '', words)\n",
    "        words = re.sub('[%s]' % re.escape(string.punctuation), '', words)\n",
    "        words = re.sub('\\n', '', words)\n",
    "        words = re.sub('\\w*\\d\\w*', '', words)\n",
    "        words = [word for word in words.split(' ') if words not in self.stopword]\n",
    "        words=\" \".join(words)\n",
    "        words = [self.stemmer.stem(words) for word in words.split(' ')]\n",
    "        words=\" \".join(words)\n",
    "\n",
    "        return words\n",
    "    \n",
    "\n",
    "    def clean_and_transform(self):\n",
    "\n",
    "        data = pd.read_csv(os.path.join(self.config.data_path,\"data.csv\"))\n",
    "        \n",
    "        data[\"Sentiment\"].replace({'neutral':0},inplace=True)\n",
    "        data[\"Sentiment\"].replace({'positive':1}, inplace = True)\n",
    "        data[\"Sentiment\"].replace({'negative':2}, inplace = True)\n",
    "        \n",
    "        data['Sentence'] = data['Sentence'].apply(self.data_cleaning)\n",
    "\n",
    "        data.to_csv(os.path.join(self.config.root_dir,'main_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-17 20:18:29,466: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-17 20:18:29,468: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-17 20:18:29,469: INFO: common: created directory at: artifacts]\n",
      "[2024-03-17 20:18:29,471: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2747/869471485.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[\"Sentiment\"].replace({'neutral':0},inplace=True)\n",
      "/tmp/ipykernel_2747/869471485.py:31: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[\"Sentiment\"].replace({'negative':2}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.clean_and_transform()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e9dbabff4adad09650c0c6c8c8db1a5bd7918c85351d40ef841a7bc40c071c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
